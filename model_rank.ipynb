{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('preprocess/train.csv')\n",
    "val_df = pd.read_csv('preprocess/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msno, song_id 编码\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode_label(train_df, val_df, field):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train_df[field].unique()) + list(val_df[field].unique()))\n",
    "    train_df[field] = le.transform(train_df[field])\n",
    "    val_df[field] = le.transform(val_df[field])\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df  = encode_label(train_df, val_df, 'msno')\n",
    "train_df, val_df = encode_label(train_df, val_df, 'song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config    \n",
    "        self.input_fields = config['input_fields']\n",
    "        self.dnn_units = config['dnn_units']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.batch_size = config['batch_size']\n",
    "    \n",
    "        with tf.variable_scope('inputs', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.inputs = {}\n",
    "            for field in self.input_fields:\n",
    "                self.inputs[field] = tf.placeholder(dtype=np.float32, shape=[None, 1], name=field.replace(' ', '_'))\n",
    "            self.labels = tf.placeholder(dtype=tf.float32, name='labels')\n",
    "                    \n",
    "        # deep \n",
    "        features = [self.inputs[f] for f in self.inputs]\n",
    "        self.feature_concat = tf.concat(features, axis=1)\n",
    "        \n",
    "        layer = self.feature_concat\n",
    "        for unit in self.dnn_units:\n",
    "            layer = tf.layers.dense(layer, units=unit, activation=tf.nn.relu)\n",
    "            \n",
    "        self.logit = tf.layers.dense(layer, units=1, activation=None)\n",
    "        \n",
    "        self.outputs = tf.nn.sigmoid(self.logit)\n",
    "        \n",
    "        # loss\n",
    "        self.loss = tf.losses.log_loss(self.labels, self.outputs)\n",
    "        optmizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        self.train_op = optmizer.minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    def feed_feature_and_label(self, df):\n",
    "        feeds = {}\n",
    "        for field in df.columns:\n",
    "            if field in self.input_fields:\n",
    "                feeds[self.inputs[field]] = df[field].astype(np.float32).values.reshape([-1, 1])\n",
    "        feeds[self.labels] = df['target'].astype(np.float32).values.reshape([-1, 1])\n",
    "        return feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def format_log(step, eval_info):\n",
    "    time_info = time.strftime('%Y-%m-%d %X', time.localtime())\n",
    "    eval_info_str = ', '.join([k + \": \" + str(v) for k, v in eval_info.items()])\n",
    "    log_info = time_info + \" iteration \" + str(step) + \" \" + eval_info_str\n",
    "    return log_info\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    checkpoint_dir = os.path.join(config['ckpt_dir'], config['model_name'])    \n",
    "    model = config['model']\n",
    "    train_df = config['train_df']\n",
    "    val_df = config['val_df']\n",
    "    \n",
    "    train_loss = []\n",
    "    gpu_config = tf.ConfigProto()\n",
    "    gpu_config.gpu_options.allow_growth = True\n",
    "    with tf.train.MonitoredTrainingSession(\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        config=gpu_config\n",
    "    ) as session:\n",
    "        for epoch_index in range(config['epoch']):\n",
    "            batch_size = config['batch_size']\n",
    "            \n",
    "            # train\n",
    "            for i, df in train_df.sample(frac=1).groupby(np.arange(train_df.shape[0]) // batch_size):\n",
    "                feeds = model.feed_feature_and_label(df)\n",
    "                session.run(model.train_op, feed_dict=feeds)\n",
    "                if i % 100 == 0:\n",
    "                    loss, step = session.run([model.loss, model.global_step], feed_dict=feeds)\n",
    "                    train_loss.append((step, loss))\n",
    "                    eval_info = {'loss/train': loss}\n",
    "                    log_info = format_log(step, eval_info)\n",
    "                    print(log_info)\n",
    "            \n",
    "            # validation\n",
    "            preds_all, label_all = [], []\n",
    "            loss_all = []\n",
    "            for i, df in val_df.groupby(np.arange(val_df.shape[0]) // batch_size):\n",
    "                feeds = model.feed_feature_and_label(df)\n",
    "                pred, label, loss = session.run([model.outputs, model.labels, model.loss], feed_dict=feeds)\n",
    "                preds_all.append(pred)\n",
    "                label_all.append(label)\n",
    "                loss_all.append(loss)\n",
    "            auc = roc_auc_score(np.concatenate(label_all), np.concatenate(preds_all))                \n",
    "            eval_info = {'loss/val': loss, 'auc': auc}\n",
    "            log_info = format_log(step, eval_info)\n",
    "            print(log_info)\n",
    "            \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = {\n",
    "    'model_name': 'dnn_pretrain',\n",
    "    'ckpt_dir': 'checkpoint',\n",
    "    'epoch': 1,\n",
    "    'embedding_size': 4,\n",
    "    'dnn_units': [128, 64, 32],\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024\n",
    "}\n",
    "train_df.columns = [c.replace(' ', '_') for c in train_df.columns]\n",
    "train_df.columns = [c.replace(' ', '_') for c in train_df.columns]\n",
    "\n",
    "input_fields = [field for field in train_df_svd.columns if field not in [\n",
    "    'target', 'msno', 'song_id', 'source_system_tab','source_screen_name', \\\n",
    "        'source_type','language','city','gender','registered_via']]\n",
    "\n",
    "config['input_fields'] = input_fields\n",
    "\n",
    "config['model'] = DNN(config=config)\n",
    "config['train_df'] = train_df\n",
    "config['val_df'] = val_df\n",
    "\n",
    "\n",
    "\n",
    "train_loss = train(config)\n",
    "t = list(zip(*train_loss))\n",
    "plt.plot(t[0], t[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd分解\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "label, user_idx, song_idx = train_df['target'].values, train_df['msno'].values, train_df['song_id'].values\n",
    "\n",
    "song_user_matrix = csr_matrix((label, (song_idx, user_idx)))\n",
    "\n",
    "dimension = 16\n",
    "svd = TruncatedSVD(n_components=dimension)\n",
    "song_vec = svd.fit_transform(song_user_matrix)\n",
    "\n",
    "song_vec_pretrain = pd.DataFrame(np.concatenate([np.reshape(range(len(song_vec)), [-1, 1]), song_vec], axis=1), \n",
    "                                 columns=['song_id'] + ['song_svd_' + str(c) for c in range(dimension)])\n",
    "train_df_svd = train_df.merge(song_vec_pretrain, left_on='song_id', right_on='song_id', how='left')\n",
    "val_df_svd = val_df.merge(song_vec_pretrain, left_on='song_id', right_on='song_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "config = {\n",
    "    'model_name': 'dnn_pretrain',\n",
    "    'ckpt_dir': 'checkpoint',\n",
    "    'epoch': 1,\n",
    "    'embedding_size': 4,\n",
    "    'dnn_units': [128, 64, 32],\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024\n",
    "}\n",
    "train_df_svd.columns = [c.replace(' ', '_') for c in train_df_svd.columns]\n",
    "train_df_svd.columns = [c.replace(' ', '_') for c in train_df_svd.columns]\n",
    "\n",
    "input_fields = [field for field in train_df_svd.columns if field not in [\n",
    "    'target', 'msno', 'song_id', 'source_system_tab','source_screen_name', \\\n",
    "        'source_type','language','city','gender','registered_via']]\n",
    "\n",
    "config['input_fields'] = input_fields\n",
    "\n",
    "config['model'] = DNNPretrain(config=config)\n",
    "config['train_df'] = train_df_svd\n",
    "config['val_df'] = val_df_svd\n",
    "\n",
    "\n",
    "\n",
    "train_loss = train(config)\n",
    "t = list(zip(*train_loss))\n",
    "plt.plot(t[0], t[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DNNEnd2End(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config    \n",
    "        self.embedding_fields = config['embedding_fields']\n",
    "        self.input_fields = config['input_fields']\n",
    "        self.embedding_vocab_size = config['embedding_vocab_size']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.dnn_units = config['dnn_units']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.batch_size = config['batch_size']\n",
    "    \n",
    "        with tf.variable_scope('inputs', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.inputs = {}\n",
    "            for field in self.input_fields:\n",
    "                self.inputs[field] = tf.placeholder(dtype=np.float32, shape=[None, 1], name=field.replace(' ', '_'))\n",
    "            for field in self.embedding_fields:\n",
    "                self.inputs[field] = tf.placeholder(dtype=np.int64, shape=[None, 1], name=field.replace(' ', '_'))\n",
    "            self.labels = tf.placeholder(dtype=tf.float32, name='labels')\n",
    "            \n",
    "        with tf.variable_scope('embeddings', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.embedding_matrix_dict = {}\n",
    "            self.embedding_dict = {}\n",
    "            for field in self.embedding_fields:\n",
    "                self.embedding_matrix_dict[field] = tf.get_variable(name=field,shape=[self.embedding_vocab_size[field], self.embedding_size])\n",
    "                self.embedding_dict[field] = tf.nn.embedding_lookup(self.embedding_matrix_dict[field], self.inputs[field])\n",
    "                    \n",
    "        # deep \n",
    "        features = [self.inputs[f] for f in self.inputs if f not in self.embedding_dict] + [tf.squeeze(e, axis=1) for e in self.embedding_dict.values()]\n",
    "        self.feature_concat = tf.concat(features, axis=1)\n",
    "        \n",
    "        layer = self.feature_concat\n",
    "        for unit in self.dnn_units:\n",
    "            layer = tf.layers.dense(layer, units=unit, activation=tf.nn.relu)\n",
    "            \n",
    "        self.logit = tf.layers.dense(layer, units=1, activation=None)\n",
    "        \n",
    "        self.outputs = tf.nn.sigmoid(self.logit)\n",
    "        \n",
    "        # loss\n",
    "        self.loss = tf.losses.log_loss(self.labels, self.outputs)\n",
    "        optmizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        self.train_op = optmizer.minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    def feed_feature_and_label(self, df):\n",
    "        feeds = {}\n",
    "        for field in df.columns:\n",
    "            if field in self.embedding_fields:\n",
    "                feeds[self.inputs[field]] = df[field].astype(np.int64).values.reshape([-1, 1])\n",
    "            elif field in self.input_fields:\n",
    "                feeds[self.inputs[field]] = df[field].astype(np.float32).values.reshape([-1, 1])\n",
    "        feeds[self.labels] = df['target'].astype(np.float32).values.reshape([-1, 1])\n",
    "        return feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "config = {\n",
    "    'model_name': 'dnn_end2end',\n",
    "    'ckpt_dir': 'checkpoint',\n",
    "    'epoch': 1,\n",
    "    'embedding_size': 16,\n",
    "    'dnn_units': [128, 64, 32],\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'train_df': train_df,\n",
    "    'val_df': val_df\n",
    "}\n",
    "train_df.columns = [c.replace(' ', '_') for c in train_df.columns]\n",
    "val_df.columns = [c.replace(' ', '_') for c in val_df.columns]\n",
    "\n",
    "input_fields = [field for field in train_df.columns if field not in [\n",
    "    'target', 'msno', 'song_id', 'source_system_tab','source_screen_name', \\\n",
    "        'source_type','language','city','gender','registered_via']]\n",
    "\n",
    "config['input_fields'] = input_fields\n",
    "config['embedding_fields'] = ['song_id', 'source_system_tab','source_screen_name', 'source_type']\n",
    "config['embedding_vocab_size'] = {}\n",
    "for field in config['embedding_fields']:\n",
    "    vocab_size = len(set(list(train_df[field].unique()) + list(val_df[field].unique())))\n",
    "    config['embedding_vocab_size'][field] = vocab_size\n",
    "\n",
    "config['model'] = DNNEnd2End(config=config)\n",
    "\n",
    "\n",
    "\n",
    "train_loss = train(config)\n",
    "t = list(zip(*train_loss))\n",
    "plt.plot(t[0], t[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DeepFM(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config    \n",
    "        self.embedding_fields = config['embedding_fields']\n",
    "        self.input_fields = config['input_fields']\n",
    "        self.embedding_vocab_size = config['embedding_vocab_size']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.dnn_units = config['dnn_units']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.batch_size = config['batch_size']\n",
    "    \n",
    "        with tf.variable_scope('inputs', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.inputs = {}\n",
    "            for field in self.input_fields:\n",
    "                self.inputs[field] = tf.placeholder(dtype=np.float32, shape=[None, 1], name=field.replace(' ', '_'))\n",
    "            for field in self.embedding_fields:\n",
    "                self.inputs[field] = tf.placeholder(dtype=np.int64, shape=[None, 1], name=field.replace(' ', '_'))\n",
    "            self.labels = tf.placeholder(dtype=tf.float32, name='labels')\n",
    "            \n",
    "        with tf.variable_scope('embeddings', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.embedding_matrix_dict = {}\n",
    "            self.embedding_dict = {}\n",
    "            for field in self.embedding_fields:\n",
    "                self.embedding_matrix_dict[field] = tf.get_variable(name=field,shape=[self.embedding_vocab_size[field], self.embedding_size])\n",
    "                self.embedding_dict[field] = tf.nn.embedding_lookup(self.embedding_matrix_dict[field], self.inputs[field])\n",
    "                    \n",
    "        # deep \n",
    "        features = [self.inputs[f] for f in self.inputs if f not in self.embedding_dict] + [tf.squeeze(e, axis=1) for e in self.embedding_dict.values()]\n",
    "        self.feature_concat = tf.concat(features, axis=1)\n",
    "        \n",
    "        layer = self.feature_concat\n",
    "        for unit in self.dnn_units:\n",
    "            layer = tf.layers.dense(layer, units=unit, activation=tf.nn.relu)\n",
    "            \n",
    "        self.logit = tf.layers.dense(layer, units=1, activation=None)\n",
    "        \n",
    "        # fm \n",
    "        fm_linear_input = tf.squeeze(tf.concat(list(self.embedding_dict.values()), axis=2), axis=1)\n",
    "        self.logit_fm_linear = tf.layers.dense(fm_linear_input, units=1, activation=None)\n",
    "        \n",
    "        fm_cross_input = tf.concat(list(self.embedding_dict.values()), axis=1)\n",
    "        square_of_sum = tf.square(tf.reduce_sum(fm_cross_input, axis=1, keep_dims=True))\n",
    "        sum_of_square = tf.reduce_sum(fm_cross_input * fm_cross_input, axis=1, keep_dims=True)\n",
    "        self.logit_fm_corss = 0.5 * tf.reduce_sum(square_of_sum - sum_of_square, axis=2, keep_dims=False)\n",
    "        \n",
    "        self.outputs = tf.nn.sigmoid(self.logit + self.logit_fm_linear + self.logit_fm_corss)\n",
    "        \n",
    "        # loss\n",
    "        self.loss = tf.losses.log_loss(self.labels, self.outputs)\n",
    "        optmizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        self.train_op = optmizer.minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    def feed_feature_and_label(self, df):\n",
    "        feeds = {}\n",
    "        for field in df.columns:\n",
    "            if field in self.embedding_fields:\n",
    "                feeds[self.inputs[field]] = df[field].astype(np.int64).values.reshape([-1, 1])\n",
    "            elif field in self.input_fields:\n",
    "                feeds[self.inputs[field]] = df[field].astype(np.float32).values.reshape([-1, 1])\n",
    "        feeds[self.labels] = df['target'].astype(np.float32).values.reshape([-1, 1])\n",
    "        return feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "config = {\n",
    "    'model_name': 'deepfm',\n",
    "    'ckpt_dir': 'checkpoint',\n",
    "    'epoch': 1,\n",
    "    'embedding_size': 16,\n",
    "    'dnn_units': [128, 64, 32],\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'train_df': train_df,\n",
    "    'val_df': val_df\n",
    "}\n",
    "train_df.columns = [c.replace(' ', '_') for c in train_df.columns]\n",
    "val_df.columns = [c.replace(' ', '_') for c in val_df.columns]\n",
    "\n",
    "input_fields = [field for field in train_df.columns if field not in [\n",
    "    'target', 'msno', 'song_id', 'source_system_tab','source_screen_name', \\\n",
    "        'source_type','language','city','gender','registered_via']]\n",
    "\n",
    "config['input_fields'] = input_fields\n",
    "config['embedding_fields'] = ['song_id', 'source_system_tab','source_screen_name', 'source_type']\n",
    "config['embedding_vocab_size'] = {}\n",
    "for field in config['embedding_fields']:\n",
    "    vocab_size = len(set(list(train_df[field].unique()) + list(val_df[field].unique())))\n",
    "    config['embedding_vocab_size'][field] = vocab_size\n",
    "\n",
    "config['model'] = DeepFM(config=config)\n",
    "\n",
    "\n",
    "train_loss = train(config)\n",
    "t = list(zip(*train_loss))\n",
    "plt.plot(t[0], t[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9804c6224cca3325f6fd8d904cd009693fc00c49e59fc1f9b5ed376dfc7e487"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
